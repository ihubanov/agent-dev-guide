## How it Works - Summary Flow

The personal-agent processes user requests and interacts with the Large Language Model (LLM) in a straightforward, streaming-first manner. Here's a step-by-step summary of the request lifecycle:

1.  **User Request**: A client application sends an HTTP `POST` request to the `/prompt` endpoint of the personal-agent. The request body contains a JSON payload, which must include a `messages` array representing the conversation history or the user's current query.

2.  **Server Receives Request (`src/index.ts`)**: The Express.js server, defined in `src/index.ts`, receives this incoming request. The routing mechanism directs it to the `handlePrompt` asynchronous function.

3.  **Initiate LLM Interaction (`handlePrompt` calls `prompt`)**: The `handlePrompt` function extracts the `messages` (and any other relevant data) from the request payload. It then calls the `prompt` function, which is imported from `src/prompt/index.ts`, passing the payload to it.

4.  **Processing Inside `prompt` Function (`src/prompt/index.ts`)**:
    *   **System Prompt Retrieval**: The `prompt` function first determines the system prompt to be used. It prioritizes the `SYSTEM_PROMPT` environment variable. If this variable is not set or is empty, it falls back to reading the content of the `src/system-prompt.txt` file.
    *   **Message Assembly**: The retrieved system prompt (as a "system" role message) is then prepended to the array of user/assistant messages received in the payload. This combined list forms the complete conversational context provided to the LLM.
    *   **Nature of Interaction (Direct API Call)**: It's important to note that the interaction with the LLM is a direct API call to an OpenAI-compatible service. The "chain" of interaction is simply the ordered list of messages (system prompt + conversation history) sent to the LLM. The agent does not use a complex chaining library like LangChain for this core interaction; it's a more direct and lightweight approach.
    *   **API Request to LLM**: An API request is made to the configured LLM using `openAI.chat.completions.create`. Crucially, this request includes `stream: true` to enable streaming responses and `temperature: 0` / `seed: 42` for consistent outputs.

5.  **LLM Response Generation**: The LLM processes the input messages and begins generating a response, sending it back as a series of chunks or tokens due to the streaming request.

6.  **Stream Production (`src/prompt/index.ts`)**: As the `prompt` function receives these chunks from the LLM, it wraps them in a `ReadableStream`. Each chunk (an `OpenAI.ChatCompletionChunk` object) is formatted as a Server-Sent Event (SSE) string, typically `data: {JSON_chunk}\n\n`, and then encoded to a `Uint8Array` before being enqueued into the stream.

7.  **Streaming to Client (`handlePrompt` in `src/index.ts`)**:
    *   **SSE Headers**: Back in the `handlePrompt` function, appropriate HTTP headers are set on the response to the client (e.g., `Content-Type: text/event-stream`, `Cache-Control: no-cache`, `Connection: keep-alive`) to indicate an SSE stream.
    *   **Forwarding Chunks**: The function reads the SSE-formatted `Uint8Array` chunks from the `ReadableStream` provided by the `prompt` function. Each chunk is immediately written to the HTTP response.
    *   **Flushing**: `res.flush()` is called (if available) after writing each chunk. This ensures that the data is sent to the client without unnecessary buffering, maintaining the real-time nature of the stream.

8.  **Client Processing**: The client application receives these SSE chunks. It can then parse the `data` field (which contains the JSON string of the LLM chunk) and update its UI or process the information as it arrives.

9.  **Stream Completion**: Once the LLM has finished generating its full response, the `ReadableStream` in the `prompt` function is closed. This signals the `handlePrompt` function that the stream is complete. `handlePrompt` then writes a final SSE message, `data: [DONE]\n\n`, to the client and closes the HTTP response. This clearly indicates to the client that no more data will be sent.

This flow ensures that the user receives feedback from the agent as quickly as possible, with messages appearing incrementally as they are generated by the LLM.
